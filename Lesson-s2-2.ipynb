{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Математика нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Нейронная сеть со скрытыми слоями\n",
    "<img src=\"images/simple_neural_net.png\" alt=\"Synaps\" height=30% width=30%>\n",
    "$$ H = f_1(W_1*I)$$\n",
    "$$ O = f_2(W_2*H)$$\n",
    "\n",
    "$W_1$ и $W_2$ - матрицы весов, обучаемые параметры  \n",
    "$I$ - вектор входных признаков  \n",
    "$O$ - вектор выхода  \n",
    "$H$ - вектор скрытого состояния  \n",
    "$f$ - функция активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Геометрическая интерпретация\n",
    "\n",
    "$ y = W*x + b $ - разделяющая гиперплоскость для каждой координаты $y_i$. \n",
    "\n",
    "<img src=\"images/II2/hyper.png\" alt=\"Synaps\" height=30% width=30%>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Градиентный спуск (Оптимизация модели)\n",
    "\n",
    "**Градиентный спуск** — метод нахождения локального экстремума (минимума или максимума) функции с помощью движения вдоль градиента.\n",
    "\n",
    "<img src=\"images/II2/grad.png\" alt=\"Synaps\" height=30% width=30%>\n",
    "$ x_t $ - координата, отражающая значения всех весов модели ($w_i$, $b_j$) на t итерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обратное распространение ошибки. Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$f(x,w) = 1 + e^{w_1x+w_0}$$\n",
    "<img src=\"images/II2/back1_1.png\" alt=\"Synaps\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Начальное состояение весов и входного значения\n",
    "<img src=\"images/II2/back1_2.png\" alt=\"Synaps\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Посчитаем значения по прямому проходу\n",
    "<img src=\"images/II2/back1_3.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Для удобства обозначим узлы (подфункции)\n",
    "<img src=\"images/II2/back1_4.png\" alt=\"Synaps\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Обратное распространение. Пусть df = 1. Тогда \n",
    "$$dc = {df \\over dc}df = (c+1)'_{c} * 1 = 1$$\n",
    "$$db = {dc \\over db}dc = (e^b)'_{b} * 1 = e^3 = 20$$\n",
    "<img src=\"images/II2/back1_5.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$dw_0 = {db \\over dw_0}db = (a+w_0)'_{w_0} * 20 = 20$$\n",
    "$$da = {db \\over da}db = (a+w_0)'_{a} * 20 = 20$$\n",
    "<img src=\"images/II2/back1_6.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$dw_1 = {da \\over dw_1}da = (x*w_1)'_{w_1} * 20 = x*20 = 20$$\n",
    "$$dx = {da \\over dx}da = (x*w_1)'_{x} * 20 = w_1*20 = 40$$\n",
    "<img src=\"images/II2/back1_7.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обратное распространение ошибки. Пример с матрицами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$f={1 \\over 2} ||X*W||^2_2$$\n",
    "<img src=\"images/II2/back2_1.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Начальное состояение весов и входного значения\n",
    "<img src=\"images/II2/back2_2.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Посчитаем значения по прямому проходу\n",
    "<img src=\"images/II2/back2_3.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Для удобства обозначим узлы (подфункции)  \n",
    "<img src=\"images/II2/back2_4.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Обратное распространение. Пусть df = 1. Тогда \n",
    "$$db = {df \\over db}df = ({1 \\over 2}b)'_{b} * 1 = 0.5$$\n",
    "$$da_{ij} = {db \\over da_{ij}}db = (a_{ij}^2)'_{a_{ij}} * 0.5 = a_{ij} $$\n",
    "или $$\\nabla_a f = a$$\n",
    "\n",
    "<img src=\"images/II2/back2_5.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\nabla_w f = X^T \\nabla_a f$$\n",
    "\n",
    "<img src=\"images/II2/back2_6.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Кросс энтропия (Фунция потерь)\n",
    "\n",
    "$y(x_i) = y_i$ - класс объекта $x_i$  \n",
    "$p(y_i|x_i)$ - вероятнсть принадлежности объекта $x_i$ к классу $y_i$  \n",
    "Цель: максимизировать вероятность принадлежности элемента правильному классу $p(y_1|x_1), p(y_2|x_2), ...$ по w, b.  \n",
    "Следовательно, максимизируем произведение $$P = П_ip(y_i|x_i)$$ или же (для удобства) минимизируем \n",
    "$$-ln(P) = \\Sigma_iln(p(y_i|x_i))$$\n",
    "\n",
    "\n",
    "**Бинарная кросс энтропия**\n",
    "$$\n",
    "CE = -y_1log(pred_1)-(1-y_1)log(1-pred_1)\n",
    "$$\n",
    "**Категориальная кросс энтропия**\n",
    "$$\n",
    "CE = -\\sum_j^Cy_jlog(pred_j)\n",
    "$$\n",
    "*C - число классов*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"images/II2/cross_entr.png\" alt=\"Synaps\" height=80% width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Регуляризация\n",
    "\n",
    "Одной из основных проблем машинного обучения является проблема переобучения (overfitting), когда модель в погоне за минимизацией затрат на обучение теряет способность к обобщению. Существует простой способ держать переобучение под контролем — метод dropout (зануление случайных весов в процессе обучения).  \n",
    "\n",
    "\n",
    "Но есть и другие регуляризаторы, которые можно применить к сети. Возможно, самый популярный из них — $L_2$-регуляризация (также называемая сокращением весов, англ. weight decay), которая использует более прямой подход к регуляризации, чем dropout. Обычно первопричиной переобучения является сложность модели (в смысле количества ее параметров), слишком высокая для решаемой задачи и имеющегося обучающего множества. В некотором смысле, задача регуляризатора — понизить сложность модели, сохранив количество ее параметров. $L_2$-регуляризация выполняется посредством наложения штрафов (penalising) на веса с наибольшими значениями, минимизируя их $L_2$-норму с использованием параметра λ — коэффициент регуляризации, выражающий предпочтение минимизации нормы относительно минимизации потерь на обучающем множестве.  \n",
    "\n",
    "$$ L = L_{CE} + {λ\\over2}(||w||^2_2 + ||b||^2_2) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Стохастический градиентный спуск\n",
    "\n",
    "Стохастический градиентный спуск (англ. stochastic gradient descent) − оптимизационный алгоритм, отличающийся от обычного градиентного спуска тем, что градиент оптимизируемой функции считается на каждом шаге не как сумма градиентов от каждого элемента выборки, а как градиент от одного, случайно выбранного элемента, или как сумма градиентов подвыборки batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Граф вычислений\n",
    "<img src=\"images/II2/graph.png\" alt=\"Synaps\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример обучения сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Dense, Conv2D, Input, Flatten, MaxPool2D\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "              'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print('Train:', x_train.shape, y_train.shape)\n",
    "print('Test:', x_test.shape, y_test.shape)\n",
    "\n",
    "plt.figure(figsize=(15, 1.5))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.title(labels[y_train[i, 0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "inp = Input(shape=(32, 32, 3))\n",
    "layer = Conv2D(filters=8, kernel_size=(3, 3), padding='same')(inp)\n",
    "layer = MaxPool2D(pool_size=(2, 2))(layer)\n",
    "layer = Conv2D(filters=16, kernel_size=(3, 3), padding='same')(layer)\n",
    "layer = MaxPool2D(pool_size=(2, 2))(layer)\n",
    "\n",
    "layer = Flatten()(layer)\n",
    "layer = Dense(units=128)(layer)\n",
    "out = Dense(units=10, activation='softmax')(layer)\n",
    "\n",
    "model = Model(inp, out)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Подготовка данных. Разделение на классы и масштабирование\n",
    "y_train_cl = to_categorical(y_train, num_classes=10)\n",
    "y_test_cl = to_categorical(y_test, num_classes=10)\n",
    "x_train_scaled = x_train / 255.\n",
    "x_test_scaled = x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Для визуализации графа и процесса обучения в tensordflow и keras используется инструмент tensorboard\n",
    "from keras.callbacks import TensorBoard\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs/example1')\n",
    "# from datetime import datetime\n",
    "# logdir = \"logs/example1/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tensorboard_callback = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Обучение\n",
    "history = model.fit(x_train_scaled, y_train_cl, validation_data=(x_test_scaled, y_test_cl), \n",
    "                    batch_size=32, epochs=10, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "tensorbaord запускается с помощью команды\n",
    "```\n",
    "tensorboard --logdir <logs path>\n",
    "```\n",
    "По умолчанию доступно через браузер по адресу на **localhost:6006**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Задания  \n",
    "\n",
    "1. Для данных MNIST построить нейронную сеть с выводом логов для tensorboard. Посмотреть в изменение логов в процессе обучения.\n",
    "2. Сравнить поведение процесса обучения различными оптимизаторами для модели (графики истории).\n",
    "3. Изменить структуру сети так, чтобы модель переобучалась (для этого можно использовать тест как валидацию). Посмотреть, как это будут выглядеть графики обучения. (Если не удаётся переобучить, то уменьшить размер тренировочной выборки).  \n",
    "4. Использовать аугментацию при обучении. Сравнить графики.  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
