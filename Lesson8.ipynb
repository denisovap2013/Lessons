{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Что такое нейронная сеть\n",
    "\n",
    "**Нейронная сеть** — это последовательность нейронов, соединенных между собой синапсами. Структура нейронной сети пришла в мир программирования прямиком из биологии. Благодаря такой структуре, машина обретает способность анализировать и даже запоминать различную информацию. Нейронные сети также способны не только анализировать входящую информацию, но и воспроизводить ее из своей памяти.  \n",
    "\n",
    "![Нейрон](http://aboutyourself.ru/assets/sinaps.jpg)\n",
    "\n",
    "**Аксон** — длинный отросток нейрона. Приспособлен для проведения возбуждения и информации от тела нейрона к нейрону или от нейрона к исполнительному органу.  \n",
    "**Дендриты** — короткие и сильно разветвлённые отростки нейрона, служащие главным местом для образования влияющих на нейрон возбуждающих и тормозных синапсов (разные нейроны имеют различное соотношение длины аксона и дендритов), и которые передают возбуждение к телу нейрона. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Перцептрон\n",
    "![Искуственный нейрон](https://upload.wikimedia.org/wikipedia/commons/5/57/Artificial_Neuron_Scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "X - входящий вектор признаков  \n",
    "W - веса модели  \n",
    "Y - результат модели  \n",
    "F - функция активации\n",
    "\n",
    "**<center>Смещение</center>**\n",
    "$$\n",
    "X_0 = 1\n",
    "$$\n",
    "$$\n",
    "S = \\sum_{i=0}^nX_iw_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Полносвязная неронная сеть\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/1200px-Neural_network.svg.png\" alt=\"FCNeuralNetwork\" width=30% height=30%>\n",
    "Схема простой нейросети. Зелёным цветом обозначены входные нейроны, голубым — скрытые нейроны, жёлтым — выходной нейрон\n",
    "\n",
    "$$\n",
    "\\vec {h_t} = W_h \\vec x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec h = F_h(\\vec{h_t})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec {y_t} = W_y \\vec h\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec y = F_y(\\vec{y_t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Функция активации\n",
    "## Сигмоида\n",
    "<img src=\"https://miro.medium.com/max/610/1*Noj6FwSMKHV9R_wSbnTKvA.png\" width=30% height=30% />\n",
    "$$\n",
    "F(x) = {1 \\over 1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Гиперболический тангенс\n",
    "<img src=\"https://miro.medium.com/max/514/1*TLcyVwbgfrNm3XCWW4rFzg.png\" width=30% height=30% />\n",
    "\n",
    "$$\n",
    "F(x) = {e^{2x}-1 \\over e^{2x}+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ReLU (rectified linear unit)\n",
    "<img src=\"https://miro.medium.com/max/512/1*yDTcsgBV3pcUvXFVwE8kZQ.png\" width=30% height=30% />\n",
    "$$\n",
    "F(x) = max(0, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Softmax\n",
    "\n",
    "Применяется в тех случаях, когда необходимо, чтобы сумма элементов была равно 1, а каждый элемент принадлежал интервалу \\[0; 1\\] (для задач классификации)\n",
    "$$\n",
    "F(x) = {e^{x} \\over \\sum_j e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Метод Обратного Распространения\n",
    "<img src=\"https://habrastorage.org/files/dad/168/f54/dad168f54a2d4cf0b6508200eda50eef.png\" alt=\"Example\" height=30% width=30%>\n",
    "Ошибка выходного нейрона\n",
    "$$\n",
    "\\delta_{output} = (out_{pred} - out_{y})f'(in)\n",
    "$$\n",
    "Ошибка внутренного нейрона\n",
    "$$\n",
    "\\delta_{hid} = f'(in)\\sum_i(w_i\\delta_i)\n",
    "$$\n",
    "Градиент ошибки нейрона\n",
    "$$\n",
    "grad_a^b = out_a*\\delta_b\n",
    "$$\n",
    "Изменение весов\n",
    "$$\n",
    "\\Delta w_t = E*grad_a^b+\\alpha*\\Delta w_{t-1}\n",
    "$$\n",
    "\n",
    "*E* - скорость обучения  \n",
    "$\\alpha$ - момент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Значение момента\n",
    "![Momentum](https://habrastorage.org/files/4d9/684/061/4d96840618dc44768e57e892033a119a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Функция потерь\n",
    "### Регрессия\n",
    "**Средняя абсолютная ошибка**\n",
    "$$\n",
    "MAE = {1\\over n}\\sum_{i=1}^n|y_i - pred_i|\n",
    "$$\n",
    "*n - число признаков в выходном векторе*  \n",
    "*pred - предсказанный вектор*\n",
    "  \n",
    "**Средняя квадратичная ошибка**\n",
    "$$\n",
    "MSE = {1\\over n}\\sum_{i=1}^n(y_i - pred_i)^2\n",
    "$$\n",
    "**Средняя абсолютная процентная ошибка**\n",
    "$$\n",
    "MAPE = {100\\% \\over n}\\sum_{i=1}^n{|y_i - pred_i|\\over y_i}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Классификация\n",
    "**Бинарная кросс энтропия**\n",
    "$$\n",
    "CE = -y_1log(pred_1)-(1-y_1)log(1-pred_1)\n",
    "$$\n",
    "**Категориальная кросс энтропия**\n",
    "$$\n",
    "CE = -\\sum_j^Cy_jlog(pred_j)\n",
    "$$\n",
    "*C - число классов*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Сверточные нейронные сети\n",
    "\n",
    "![Архитектура](https://upload.wikimedia.org/wikipedia/commons/5/55/%D0%90%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0_%D1%81%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%BE%D0%B9_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%BE%D0%B9_%D1%81%D0%B5%D1%82%D0%B8.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Слой свёртки (convolutional layer) — это основной блок свёрточной нейронной сети. Слой свёртки включает в себя для каждого канала свой фильтр, ядро свёртки которого обрабатывает предыдущий слой по фрагментам (суммируя результаты поэлементного произведения для каждого фрагмента). Весовые коэффициенты ядра свёртки (небольшой матрицы) неизвестны и устанавливаются в процессе обучения.\n",
    "\n",
    "![Свёртка](https://neurohive.io/wp-content/uploads/2018/07/2d-covolutions.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Архитектуры нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Датасет ImageNet\n",
    "ImageNet — набор данных, состоящий из более чем 15 миллионов размеченных высококачественных изображений, разделенных на 22000 категорий. Изображения были взяты из интернета и размечены вручную людьми-разметчиками \n",
    "\n",
    "<img src=\"https://alexisbcook.github.io/assets/cifar10.png\" alt=\"VGG\" height=60% width=60% />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## VGG\n",
    "\n",
    "VGG16 — одна из самых знаменитых моделей, отправленных на соревнование ILSVRC-2014. Она является улучшенной версией AlexNet, в которой заменены большие фильтры (размера 11 и 5 в первом и втором сверточном слое, соответственно) на несколько фильтров размера 3х3, следующих один за другим. Сеть VGG16 обучалась на протяжении нескольких недель при использовании видеокарт NVIDIA TITAN BLACK.\n",
    "\n",
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network-1-e1542973058418.jpg\" alt=\"VGG\" height=60% width=60% />\n",
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-2.png\" alt=\"VGG Plain\" height=50% width=50% />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ResNet\n",
    "Основой сети ResNet является Residual-блок (остаточный блок) с shortcut-соединением, через которое данные проходят без изменений. Res-блок представляет собой несколько сверточных слоев с активациями, которые преобразуют входной сигнал x в F(x). Shortcut-соединение — это тождественное преобразование x -> x.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/322621180/figure/fig2/AS:584852684410885@1516451154473/The-representation-of-model-architecture-image-for-ResNet-152-VGG-19-and-two-layered.png\" alt=\"ResBlock\" height=30% width=30% />\n",
    "<img src=\"https://neurohive.io/wp-content/uploads/2019/01/resnet-neural-e1548772388921.png\" alt=\"ResBlock\" height=30% width=30% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MobileNet V2\n",
    "\n",
    "Особенностью данной архитектуры является отсутствие max pooling-слоёв. Вместо них для снижения пространственной размерности используется свёртка с параметром stride, равным 2. Также изменение свёртки уменьшает количество операций сети\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1016/1*5iA55983nBMlQn9f6ICxKg.png\" alt=\"MobNet\" height=30% width=30% />\n",
    "\n",
    "\n",
    "<img src=\"https://habrastorage.org/webt/wl/yo/sz/wlyoszqnws58itd4ojt1cqt7sng.png\" alt=\"MobBlock\" height=30% width=30% />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# U-Net\n",
    "U-Net — это свёрточная нейронная сеть, которая была создана в 2015 году для сегментации биомедицинских изображений в отделении Computer Science Фрайбургского университета. Архитектура сети представляет собой полносвязную свёрточную сеть, модифицированную так, чтобы она могла работать с меньшим количеством примеров (обучающих образов) и делала более точную сегментацию.\n",
    "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"UNet\" height=50% width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Рекуррентные нейронные сети (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Рекуррентные нейронные сети (RNN) — вид нейронных сетей, где связи между элементами образуют направленную последовательность. Благодаря этому появляется возможность обрабатывать серии событий во времени или последовательные пространственные цепочки. В отличие от многослойных перцептронов, рекуррентные сети могут использовать свою внутреннюю память для обработки последовательностей произвольной длины. Поэтому сети RNN применимы в таких задачах, где нечто целостное разбито на части, например: распознавание рукописного текста или распознавание речи. \n",
    "<img src=\"https://habrastorage.org/web/5c8/0fa/c22/5c80fac224d449209d888d18ea1111a8.png\" alt=\"RNN\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Внутреннее устройство простых RNN\n",
    "Входной вектор и вектор внутренней памяти объединяются и отправляются на полносвязный слой с функцией активации *tanh*\n",
    "<img src=\"https://habrastorage.org/web/47d/ee6/2c3/47dee62c3af8498c946befa1f3330d90.png\" alt=\"RNN In\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM\n",
    "Долгая краткосрочная память (Long short-term memory; LSTM) – особая разновидность архитектуры рекуррентных нейронных сетей, способная к обучению долговременным зависимостям.\n",
    "<img src=\"https://habrastorage.org/web/67b/04f/73b/67b04f73b4c34ba38edfa207e09de07c.png\" alt=\"LSTM\">\n",
    "* «вентиль забывания» контролирует меру сохранения значения в памяти\n",
    "* «входной вентиль» контролирует меру вхождения нового значения в память\n",
    "* «выходной вентиль» контролирует меру того, в какой степени значение, находящееся в памяти, используется при расчёте выходной функции активации для блока"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GRU\n",
    "Управляемые рекуррентные блоки (Gated Recurrent Units, GRU) — механизм вентилей для рекуррентных нейронных сетей, представленный в 2014 году. Было установлено, что его эффективность при решении задач моделирования музыкальных и речевых сигналов сопоставима с использованием долгой краткосрочной памяти (LSTM). По сравнению с LSTM у данного механизма меньше параметров, т.к. отсутствует выходной вентиль.\n",
    "<img src=\"https://www.data-blogger.com/wp-content/uploads/2017/08/gru.png\" alt=\"GRU\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обучение с подкреплением\n",
    "Обучение с подкреплением — это метод машинного обучения, при котором происходит обучение модели, которая не имеет сведений о системе, но имеет возможность производить какие-либо действия в ней. Действия переводят систему в новое состояние и модель получает от системы некоторое вознаграждение\n",
    "\n",
    "<img src=\"https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg\" alt=\"RF\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Обучение модели\n",
    "<img src=\"https://media.giphy.com/media/PH67wPdphHPk4/giphy.gif\" alt=\"Walking\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Использование нейронных сетей в Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CIFAR10\n",
    "Датасет CIFAR10 содержит 50,000 32x32 цветных изображений для обучения, разделённых на 10 классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "labels = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "              'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print('Train:', x_train.shape, y_train.shape)\n",
    "print('Test:', x_test.shape, y_test.shape)\n",
    "\n",
    "plt.figure(figsize=(15, 1.5))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.title(labels[y_train[i, 0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Conv2D, Input, Flatten, MaxPool2D\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(32, 32, 3))\n",
    "layer = Conv2D(filters=8, kernel_size=(3, 3), padding='same')(inp)\n",
    "layer = MaxPool2D(pool_size=(2, 2))(layer)\n",
    "layer = Conv2D(filters=16, kernel_size=(3, 3), padding='same')(layer)\n",
    "layer = MaxPool2D(pool_size=(2, 2))(layer)\n",
    "\n",
    "layer = Flatten()(layer)\n",
    "layer = Dense(units=128)(layer)\n",
    "out = Dense(units=10, activation='softmax')(layer)\n",
    "\n",
    "model = Model(inp, out)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "img = plt.imread('model.png')\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Подготовка данных. Разделение на классы и масштабирование\n",
    "y_train_cl = to_categorical(y_train, num_classes=10)\n",
    "x_train_scaled = x_train / 255.\n",
    "x_test_scaled = x_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Предсказание на необученной модели\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "for i in range(3):\n",
    "    \n",
    "    probs = dict(zip(labels, y_pred[i]))\n",
    "    print(probs)\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Обучение\n",
    "history = model.fit(x_train_scaled, y_train_cl, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Предсказание на обученной модели\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "for i in range(3):\n",
    "    \n",
    "    probs = dict(zip(labels, y_pred[i]))\n",
    "    print(probs)\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST\n",
    "\n",
    "Датасет MNIST содержит 60,000 28x28 черно-белых изображений 10 цифр для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('Train:', x_train.shape, y_train.shape)\n",
    "print('Test:', x_test.shape, y_test.shape)\n",
    "\n",
    "plt.figure(figsize=(15, 1.5))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.imshow(x_train[i], cmap='gray')\n",
    "    plt.title(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ресурсы для углублённого изучения темы\n",
    "* [Видеолекции Семёна Козлова](https://www.youtube.com/channel/UCQj_dwbIydi588xrfjWSL5g/featured)\n",
    "* [Курс по машинному обучению](https://www.coursera.org/learn/machine-learning)\n",
    "* [Цикл статей об истории нейронных сетей](http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Задания\n",
    "\n",
    "1. Создайте и обучите модель для MNIST\n",
    "2. Оцените точность результата\n",
    "3. Используйте часть обучающихся данных как валидацию (аргумент у метода fit validation_split) и выведите график изменения точности (значения в переменной history)\n",
    "4. Постройте полносвязную нейронную сеть и обучите её на данных с Титаника"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Huawei Deblur Env",
   "language": "python",
   "name": "huawei-deblur"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
